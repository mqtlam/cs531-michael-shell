\section{Randomized Reflex Agent}
In the randomized reflex agent, it is possible to take several different actions in terms of the action probability distribution. We use the multinomial distribution in this case. For designing the parameters in these multinomial distributions, we use a heuristics in the light of an optimal path, which shows as dash line in figure \ref{fig:celltypes}. For a $n \times m$ map, there are $(n-1) \times m$ forward actions, $\lfloor (m-2)/2 \rfloor$ left and right turnings when no wall and dirt have
been detected. When facing walls, the ratio of turning left and turning right is $2:1$. Based on these observations, we design the if-then rules and multinomial distributions as in table \ref{tab:random}. The agent will choose an action according to the random number generated by the multinomial distribution.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|c|c|c|c|c|}
        \hline
        WALL & DIRT & HOME & FORWARD & RIGHT & LEFT & SUCK & OFF  \\ \hline
        1    & 0    & 1    & 0.0     & 0.65  & 0.33 & 0.0  & 0.02 \\ 
        1    & 1    & 0    & 0.0     & 0.0   & 0.0  & 1.0  & 0.0  \\ 
        1    & 1    & 1    & 0.0     & 0.0   & 0.0  & 1.0  & 0.0  \\ 
        1    & 0    & 0    & 0.0     & 0.67  & 0.33 & 0.0  & 0.0  \\ 
        0    & 0    & 1    & 0.9     & 0.05  & 0.05 & 0.0  & 0.0  \\ 
        0    & 1    & 0    & 0.0     & 0.0   & 0.0  & 1.0  & 0.0  \\ 
        0    & 1    & 1    & 0.0     & 0.0   & 0.0  & 1.0  & 0.0  \\ 
        0    & 0    & 0    & 0.8     & 0.1   & 0.1  & 0.0  & 0.0  \\
        \hline
    \end{tabular}
    \caption{Parameters of multinomial distributions for each situation.}\label{tab:random}
\end{table}

We prefer the agent stop at home, because this is more likely that it has cleaned as many as possible cells before coming back. Note that in practice, we didn't let the agent turn off before reaching the maxmum number of permitted actions. For a cell has dirt, we let the agent clean this cell first as the greedy strategy. When there are no wall and dirt being detected, the agent is designed to be inclined to keep forward, but the ratio is not as high as the case of optimal path. We found
that make the probability of going forward a little bit lower will achieve better performance.
